BigData\_analysis\_of\_transport\_in\_Ukraine\_SHI33


## Етап налаштування

Інструкція з налаштування середовища Spark

Цей посібник містить повний перелік кроків для налаштування робочого середовища як через Docker, так і шляхом локального встановлення Python та PySpark.

Частина 1: Робота з Docker

Встановлення Docker
Якщо у вас ще немає Docker, завантажте та встановіть його з офіційного сайту.

Перевірка готовності
Перевірте в терміналі, що все працює, за допомогою команд:

docker --version
або
docker run hello-world


Конфігураційні файли
Додайте в корінь вашої директорії необхідний файл конфігурації (за посиланням, наданим в інструкції).

Точка входу
Перевірте, що у вашому проекті в кореневій директорії є файл main.py, який буде точкою входу у вашу програму.

Збірка образу
У терміналі PyCharm/VSCode пропишіть команду для створення образу (зверніть увагу на крапку в кінці):

docker build -t my-spark-img .


Запуск програми
Використовуйте команду нижче для кожного запуску програми. Під час роботи застосунок Docker Desktop має бути відкритим:

docker run my-spark-img


Частина 2: Локальне встановлення Python та PySpark

Встановлення Python
Встановіть Python (рекомендована версія 3.8). Завантажити можна тут: python.org/downloads.

Підготовка IDE
Встановіть та налаштуйте PyCharm або VSCode.

Налаштування Java та PySpark
Для роботи PySpark необхідно встановити Java 8 або 11. Детальніше про процес налаштування можна прочитати у цьому туторіалі.

Робота в IDE
Відкрийте обрану IDE (PyCharm або VSCode) та ваш проект.

Встановлення бібліотек
У терміналі вашої IDE виконайте команду:

pip install pyspark


Фінальна перевірка
Створіть файл main.py. Спробуйте створити тестовий DataFrame і застосувати до нього метод .show(). Якщо дані відображаються належним чином — вітаємо, ви готові до роботи!

## Етап видобування
1. Було завантажено датасет локально. Після цього створено data\_loader.py у якому csv файл був прочитаний та розділений через пандас на категорії з правильним форматуванням для подальшого зручного аналізу.

2. У main.py було додано вивід DataFrame та проаналізовано коректність зчитування з data\_loader.py
===

